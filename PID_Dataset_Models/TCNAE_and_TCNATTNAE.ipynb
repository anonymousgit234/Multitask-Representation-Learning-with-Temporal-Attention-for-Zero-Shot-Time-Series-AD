{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd88a301",
   "metadata": {},
   "source": [
    "## TCNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bf23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def TCNAEModel(lookback=8, ts_dims = 137, layer1=20, layer2=6, encoding_dim=4):\n",
    "    model = Sequential([\n",
    "    TCN(input_shape=(lookback, ts_dims), nb_filters=layer1, kernel_size=3, padding='same',activation='relu', return_sequences=True),\n",
    "    Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same'),\n",
    "    Conv1D(filters=encoding_dim, kernel_size=3, activation='relu', padding='same'),\n",
    "    AveragePooling1D(pool_size=4, strides=None, padding='valid'),\n",
    "    Activation(\"linear\"),\n",
    "    UpSampling1D(size=4),\n",
    "    Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same'),\n",
    "    TCN(nb_filters=layer1, kernel_size=3, padding='same',activation='relu',return_sequences=True),\n",
    "    Dense(ts_dims, activation='sigmoid')\n",
    "])\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2']\n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "\n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/TCNAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)\n",
    "        model  = TCNAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = temporalize(train_merge_df)\n",
    "\n",
    "    model = TCNAEModel(layer1=128, layer2=64, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca98dc",
   "metadata": {},
   "source": [
    "## TCNATTNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def TCNAttnAEModel(lookback=8, n_features=137, layer1=100, layer2=64, encoding_dim = 4):\n",
    "    encoder_inputs = Input(shape=(8, n_features))\n",
    "    encoder = TCN(nb_filters=layer1, kernel_size=3, padding='same',activation='relu', return_sequences=True)(encoder_inputs)\n",
    "    encoder = Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "    encoder = Conv1D(filters=encoding_dim, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "    attention = Attention()([encoder, encoder])\n",
    "    attention = AveragePooling1D(pool_size=4, strides=None, padding='valid')(attention)\n",
    "    decoder = UpSampling1D(size=4)(attention)\n",
    "    decoder = Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same')(decoder)\n",
    "    decoder = TCN(nb_filters=layer1, kernel_size=3, padding='same',activation='relu',return_sequences=True)(decoder)\n",
    "    decoder_outputs = Dense(n_features, activation='sigmoid')(decoder)\n",
    "    autoencoder = Model(encoder_inputs, decoder_outputs)\n",
    "    autoencoder.compile(loss='mse', optimizer='adam')    \n",
    "    return autoencoder\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2'] \n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "    \n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2])) \n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/TCNATTNAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)                                               \n",
    "        model  = TCNAttnAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = temporalize(train_merge_df)\n",
    "\n",
    "    model = TCNAttnAEModel(layer1=128, layer2=64, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
