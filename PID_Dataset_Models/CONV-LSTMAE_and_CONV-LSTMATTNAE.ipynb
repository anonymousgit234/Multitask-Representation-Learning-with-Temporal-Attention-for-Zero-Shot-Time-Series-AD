{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONV-LSTMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 10:29:52.143382: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-14 10:29:52.143758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-09-14 10:29:52.143855: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-09-14 10:29:52.143945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-09-14 10:29:52.176693: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-09-14 10:29:52.176784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-09-14 10:29:52.176799: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-09-14 10:29:52.177803: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 137)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1, 64)             26368     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 32)             12416     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1, 32)             8320      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 1, 32)             8320      \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 1, 64)            6208      \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 1, 137)           26441     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,073\n",
      "Trainable params: 88,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "230/230 - 4s - loss: 0.0712 - val_loss: 0.0193 - 4s/epoch - 15ms/step\n",
      "Epoch 2/200\n",
      "230/230 - 1s - loss: 0.0127 - val_loss: 0.0145 - 1s/epoch - 5ms/step\n",
      "Epoch 3/200\n",
      "230/230 - 1s - loss: 0.0079 - val_loss: 0.0090 - 1s/epoch - 5ms/step\n",
      "Epoch 4/200\n",
      "230/230 - 1s - loss: 0.0057 - val_loss: 0.0077 - 1s/epoch - 5ms/step\n",
      "Epoch 5/200\n",
      "230/230 - 1s - loss: 0.0046 - val_loss: 0.0061 - 1s/epoch - 5ms/step\n",
      "Epoch 6/200\n",
      "230/230 - 1s - loss: 0.0040 - val_loss: 0.0053 - 1s/epoch - 5ms/step\n",
      "Epoch 7/200\n",
      "230/230 - 1s - loss: 0.0035 - val_loss: 0.0049 - 1s/epoch - 5ms/step\n",
      "Epoch 8/200\n",
      "230/230 - 1s - loss: 0.0032 - val_loss: 0.0047 - 1s/epoch - 5ms/step\n",
      "Epoch 9/200\n",
      "230/230 - 1s - loss: 0.0030 - val_loss: 0.0053 - 1s/epoch - 5ms/step\n",
      "Epoch 10/200\n",
      "230/230 - 1s - loss: 0.0029 - val_loss: 0.0044 - 1s/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "230/230 - 1s - loss: 0.0027 - val_loss: 0.0044 - 1s/epoch - 5ms/step\n",
      "Epoch 12/200\n",
      "230/230 - 1s - loss: 0.0026 - val_loss: 0.0044 - 1s/epoch - 5ms/step\n",
      "Epoch 13/200\n",
      "230/230 - 1s - loss: 0.0025 - val_loss: 0.0042 - 1s/epoch - 5ms/step\n",
      "Epoch 14/200\n",
      "230/230 - 1s - loss: 0.0024 - val_loss: 0.0036 - 1s/epoch - 5ms/step\n",
      "Epoch 15/200\n",
      "230/230 - 1s - loss: 0.0023 - val_loss: 0.0035 - 1s/epoch - 5ms/step\n",
      "Epoch 16/200\n",
      "230/230 - 1s - loss: 0.0023 - val_loss: 0.0034 - 1s/epoch - 5ms/step\n",
      "Epoch 17/200\n",
      "230/230 - 1s - loss: 0.0022 - val_loss: 0.0035 - 1s/epoch - 5ms/step\n",
      "Epoch 18/200\n",
      "230/230 - 1s - loss: 0.0022 - val_loss: 0.0033 - 1s/epoch - 5ms/step\n",
      "Epoch 19/200\n",
      "230/230 - 1s - loss: 0.0021 - val_loss: 0.0030 - 1s/epoch - 5ms/step\n",
      "Epoch 20/200\n",
      "230/230 - 1s - loss: 0.0021 - val_loss: 0.0032 - 1s/epoch - 5ms/step\n",
      "Epoch 21/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0029 - 1s/epoch - 5ms/step\n",
      "Epoch 22/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0029 - 1s/epoch - 5ms/step\n",
      "Epoch 23/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0029 - 1s/epoch - 5ms/step\n",
      "Epoch 24/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 25/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 26/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0029 - 1s/epoch - 5ms/step\n",
      "Epoch 27/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 28/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 29/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 30/200\n",
      "230/230 - 1s - loss: 0.0017 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 31/200\n",
      "230/230 - 1s - loss: 0.0017 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 32/200\n",
      "230/230 - 1s - loss: 0.0017 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 33/200\n",
      "230/230 - 1s - loss: 0.0017 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 34/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 35/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 36/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 37/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 38/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 39/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 40/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 41/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 42/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 43/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 44/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 45/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 46/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 47/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 48/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 49/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 50/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 51/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 52/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 53/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 54/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 55/200\n",
      "230/230 - 1s - loss: 0.0010 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 56/200\n",
      "230/230 - 1s - loss: 0.0010 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 57/200\n",
      "230/230 - 1s - loss: 9.7407e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 58/200\n",
      "230/230 - 1s - loss: 9.4294e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 59/200\n",
      "230/230 - 1s - loss: 9.3491e-04 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 60/200\n",
      "230/230 - 1s - loss: 9.2917e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 61/200\n",
      "230/230 - 1s - loss: 9.0369e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 62/200\n",
      "230/230 - 1s - loss: 9.0547e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 63/200\n",
      "230/230 - 1s - loss: 8.6080e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 64/200\n",
      "230/230 - 1s - loss: 8.5008e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 65/200\n",
      "230/230 - 1s - loss: 8.4818e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 66/200\n",
      "230/230 - 1s - loss: 7.7608e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 67/200\n",
      "230/230 - 1s - loss: 6.3226e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 68/200\n",
      "230/230 - 1s - loss: 6.1622e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 69/200\n",
      "230/230 - 1s - loss: 6.2151e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 70/200\n",
      "230/230 - 1s - loss: 5.9145e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 71/200\n",
      "230/230 - 1s - loss: 5.8312e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 72/200\n",
      "230/230 - 1s - loss: 5.8071e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 73/200\n",
      "230/230 - 1s - loss: 5.9876e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 74/200\n",
      "230/230 - 1s - loss: 5.6162e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "230/230 - 1s - loss: 5.5634e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 76/200\n",
      "230/230 - 1s - loss: 5.4811e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 77/200\n",
      "230/230 - 1s - loss: 5.4279e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 78/200\n",
      "230/230 - 1s - loss: 5.4808e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 79/200\n",
      "230/230 - 1s - loss: 5.2990e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 80/200\n",
      "230/230 - 1s - loss: 5.2566e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 81/200\n",
      "230/230 - 1s - loss: 5.3016e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200\n",
      "230/230 - 1s - loss: 5.1596e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 83/200\n",
      "230/230 - 1s - loss: 5.2183e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 84/200\n",
      "230/230 - 1s - loss: 5.0970e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 85/200\n",
      "230/230 - 1s - loss: 5.0054e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 86/200\n",
      "230/230 - 1s - loss: 5.0474e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 87/200\n",
      "230/230 - 1s - loss: 4.9852e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 88/200\n",
      "230/230 - 1s - loss: 4.9641e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 89/200\n",
      "230/230 - 1s - loss: 4.8279e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 90/200\n",
      "230/230 - 1s - loss: 4.8798e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 91/200\n",
      "230/230 - 1s - loss: 4.8494e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 92/200\n",
      "230/230 - 1s - loss: 4.7466e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 93/200\n",
      "230/230 - 1s - loss: 4.6703e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 94/200\n",
      "230/230 - 1s - loss: 4.7646e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 95/200\n",
      "230/230 - 1s - loss: 4.6777e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 96/200\n",
      "230/230 - 1s - loss: 4.6398e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 97/200\n",
      "230/230 - 1s - loss: 4.5743e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 98/200\n",
      "230/230 - 1s - loss: 4.5843e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 99/200\n",
      "230/230 - 1s - loss: 4.5165e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 100/200\n",
      "230/230 - 1s - loss: 4.5636e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 101/200\n",
      "230/230 - 1s - loss: 4.4238e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 102/200\n",
      "230/230 - 1s - loss: 4.4701e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 103/200\n",
      "230/230 - 1s - loss: 4.3846e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 104/200\n",
      "230/230 - 1s - loss: 4.4799e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 105/200\n",
      "230/230 - 1s - loss: 4.3955e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 106/200\n",
      "230/230 - 1s - loss: 4.2926e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 107/200\n",
      "230/230 - 1s - loss: 4.3001e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 108/200\n",
      "230/230 - 1s - loss: 4.3316e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 109/200\n",
      "230/230 - 1s - loss: 4.2865e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 110/200\n",
      "230/230 - 1s - loss: 4.3259e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 111/200\n",
      "230/230 - 1s - loss: 4.2413e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 112/200\n",
      "230/230 - 1s - loss: 4.2952e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 113/200\n",
      "230/230 - 1s - loss: 4.1316e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 114/200\n",
      "230/230 - 1s - loss: 4.2136e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 115/200\n",
      "230/230 - 1s - loss: 4.1194e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 116/200\n",
      "230/230 - 1s - loss: 4.1076e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 117/200\n",
      "230/230 - 1s - loss: 4.1132e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 118/200\n",
      "230/230 - 1s - loss: 4.1047e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 119/200\n",
      "230/230 - 1s - loss: 3.9833e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 120/200\n",
      "230/230 - 1s - loss: 3.9930e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 121/200\n",
      "230/230 - 1s - loss: 4.0182e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 122/200\n",
      "230/230 - 1s - loss: 3.9924e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 123/200\n",
      "230/230 - 1s - loss: 3.9771e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 124/200\n",
      "230/230 - 1s - loss: 3.9775e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 125/200\n",
      "230/230 - 1s - loss: 3.9915e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 126/200\n",
      "230/230 - 1s - loss: 3.9295e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 127/200\n",
      "230/230 - 1s - loss: 3.8543e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 128/200\n",
      "230/230 - 1s - loss: 3.9142e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 129/200\n",
      "230/230 - 1s - loss: 3.8577e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 130/200\n",
      "230/230 - 1s - loss: 3.9357e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 131/200\n",
      "230/230 - 1s - loss: 3.7739e-04 - val_loss: 9.4972e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 132/200\n",
      "230/230 - 1s - loss: 3.7969e-04 - val_loss: 9.9754e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 133/200\n",
      "230/230 - 1s - loss: 3.7971e-04 - val_loss: 9.7574e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 134/200\n",
      "230/230 - 1s - loss: 3.7248e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 135/200\n",
      "230/230 - 1s - loss: 3.7686e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 136/200\n",
      "230/230 - 1s - loss: 3.8059e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 137/200\n",
      "230/230 - 1s - loss: 3.6935e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 138/200\n",
      "230/230 - 1s - loss: 3.7432e-04 - val_loss: 9.8793e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 139/200\n",
      "230/230 - 1s - loss: 3.6456e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 140/200\n",
      "230/230 - 1s - loss: 3.8001e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 141/200\n",
      "230/230 - 1s - loss: 3.6373e-04 - val_loss: 9.8520e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 142/200\n",
      "230/230 - 1s - loss: 3.6940e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 143/200\n",
      "230/230 - 1s - loss: 3.6092e-04 - val_loss: 9.2149e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 144/200\n",
      "230/230 - 1s - loss: 3.5385e-04 - val_loss: 9.6127e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 145/200\n",
      "230/230 - 1s - loss: 3.5808e-04 - val_loss: 9.5790e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 146/200\n",
      "230/230 - 1s - loss: 3.5430e-04 - val_loss: 9.5114e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 147/200\n",
      "230/230 - 1s - loss: 3.6324e-04 - val_loss: 9.8915e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 148/200\n",
      "230/230 - 1s - loss: 3.5414e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 149/200\n",
      "230/230 - 1s - loss: 3.6028e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 150/200\n",
      "230/230 - 1s - loss: 3.5087e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 151/200\n",
      "230/230 - 1s - loss: 3.6139e-04 - val_loss: 9.9490e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 152/200\n",
      "230/230 - 1s - loss: 3.4552e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 153/200\n",
      "230/230 - 1s - loss: 3.4394e-04 - val_loss: 9.9893e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 154/200\n",
      "230/230 - 1s - loss: 3.4298e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 155/200\n",
      "230/230 - 1s - loss: 3.4249e-04 - val_loss: 9.3915e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 156/200\n",
      "230/230 - 1s - loss: 3.3842e-04 - val_loss: 9.7754e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 157/200\n",
      "230/230 - 1s - loss: 3.4063e-04 - val_loss: 9.7579e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 158/200\n",
      "230/230 - 1s - loss: 3.3830e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 159/200\n",
      "230/230 - 1s - loss: 3.3954e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 160/200\n",
      "230/230 - 1s - loss: 3.4657e-04 - val_loss: 9.8742e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 161/200\n",
      "230/230 - 1s - loss: 3.3408e-04 - val_loss: 9.5004e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 162/200\n",
      "230/230 - 1s - loss: 3.3548e-04 - val_loss: 9.6192e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 163/200\n",
      "230/230 - 1s - loss: 3.3358e-04 - val_loss: 9.3398e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 164/200\n",
      "230/230 - 1s - loss: 3.2864e-04 - val_loss: 9.9318e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 165/200\n",
      "230/230 - 1s - loss: 3.3697e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 166/200\n",
      "230/230 - 1s - loss: 3.3490e-04 - val_loss: 9.2358e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 167/200\n",
      "230/230 - 1s - loss: 3.2396e-04 - val_loss: 9.5180e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 168/200\n",
      "230/230 - 1s - loss: 3.3009e-04 - val_loss: 9.9659e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 169/200\n",
      "230/230 - 1s - loss: 3.2271e-04 - val_loss: 9.2823e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 170/200\n",
      "230/230 - 1s - loss: 3.2640e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 171/200\n",
      "230/230 - 1s - loss: 3.2509e-04 - val_loss: 9.5242e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 172/200\n",
      "230/230 - 1s - loss: 3.1719e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 173/200\n",
      "230/230 - 1s - loss: 3.3277e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 174/200\n",
      "230/230 - 1s - loss: 3.1906e-04 - val_loss: 9.6401e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 175/200\n",
      "230/230 - 1s - loss: 3.1688e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/200\n",
      "230/230 - 1s - loss: 3.3340e-04 - val_loss: 9.6877e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 177/200\n",
      "230/230 - 1s - loss: 3.1201e-04 - val_loss: 8.7819e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 178/200\n",
      "230/230 - 1s - loss: 3.1806e-04 - val_loss: 9.4557e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 179/200\n",
      "230/230 - 1s - loss: 3.2760e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 180/200\n",
      "230/230 - 1s - loss: 3.1037e-04 - val_loss: 9.7502e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 181/200\n",
      "230/230 - 1s - loss: 3.1382e-04 - val_loss: 9.4815e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 182/200\n",
      "230/230 - 1s - loss: 3.1260e-04 - val_loss: 9.7226e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 183/200\n",
      "230/230 - 1s - loss: 3.1973e-04 - val_loss: 9.6642e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 184/200\n",
      "230/230 - 1s - loss: 3.0675e-04 - val_loss: 9.2548e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 185/200\n",
      "230/230 - 1s - loss: 3.0206e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 186/200\n",
      "230/230 - 1s - loss: 3.0897e-04 - val_loss: 9.3985e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 187/200\n",
      "230/230 - 1s - loss: 3.1440e-04 - val_loss: 9.7154e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 188/200\n",
      "230/230 - 1s - loss: 3.0441e-04 - val_loss: 9.9774e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 189/200\n",
      "230/230 - 1s - loss: 3.0539e-04 - val_loss: 9.4906e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 190/200\n",
      "230/230 - 1s - loss: 3.0826e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 191/200\n",
      "230/230 - 1s - loss: 3.1067e-04 - val_loss: 9.4922e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 192/200\n",
      "230/230 - 1s - loss: 3.0260e-04 - val_loss: 9.6833e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 193/200\n",
      "230/230 - 1s - loss: 2.9881e-04 - val_loss: 9.8908e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 194/200\n",
      "230/230 - 1s - loss: 3.0413e-04 - val_loss: 0.0039 - 1s/epoch - 5ms/step\n",
      "Epoch 195/200\n",
      "230/230 - 1s - loss: 4.7867e-04 - val_loss: 8.6624e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 196/200\n",
      "230/230 - 1s - loss: 2.9274e-04 - val_loss: 8.7485e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 197/200\n",
      "230/230 - 1s - loss: 2.9243e-04 - val_loss: 8.9864e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 198/200\n",
      "230/230 - 1s - loss: 2.9579e-04 - val_loss: 9.3636e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 199/200\n",
      "230/230 - 1s - loss: 2.9296e-04 - val_loss: 8.9116e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 200/200\n",
      "230/230 - 1s - loss: 2.9146e-04 - val_loss: 8.9221e-04 - 1s/epoch - 5ms/step\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1, 137)]          0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1, 64)             26368     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1, 32)             12416     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1, 32)             8320      \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1, 32)             8320      \n",
      "                                                                 \n",
      " conv1d_transpose_2 (Conv1DT  (None, 1, 64)            6208      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_3 (Conv1DT  (None, 1, 137)           26441     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,073\n",
      "Trainable params: 88,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "230/230 - 4s - loss: 0.0700 - val_loss: 0.0214 - 4s/epoch - 16ms/step\n",
      "Epoch 2/200\n",
      "230/230 - 1s - loss: 0.0159 - val_loss: 0.0158 - 1s/epoch - 5ms/step\n",
      "Epoch 3/200\n",
      "230/230 - 1s - loss: 0.0093 - val_loss: 0.0110 - 1s/epoch - 5ms/step\n",
      "Epoch 4/200\n",
      "230/230 - 1s - loss: 0.0066 - val_loss: 0.0095 - 1s/epoch - 5ms/step\n",
      "Epoch 5/200\n",
      "230/230 - 1s - loss: 0.0052 - val_loss: 0.0070 - 1s/epoch - 5ms/step\n",
      "Epoch 6/200\n",
      "230/230 - 1s - loss: 0.0042 - val_loss: 0.0059 - 1s/epoch - 5ms/step\n",
      "Epoch 7/200\n",
      "230/230 - 1s - loss: 0.0036 - val_loss: 0.0052 - 1s/epoch - 5ms/step\n",
      "Epoch 8/200\n",
      "230/230 - 1s - loss: 0.0034 - val_loss: 0.0052 - 1s/epoch - 5ms/step\n",
      "Epoch 9/200\n",
      "230/230 - 1s - loss: 0.0031 - val_loss: 0.0046 - 1s/epoch - 5ms/step\n",
      "Epoch 10/200\n",
      "230/230 - 1s - loss: 0.0030 - val_loss: 0.0044 - 1s/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "230/230 - 1s - loss: 0.0028 - val_loss: 0.0046 - 1s/epoch - 5ms/step\n",
      "Epoch 12/200\n",
      "230/230 - 1s - loss: 0.0026 - val_loss: 0.0041 - 1s/epoch - 5ms/step\n",
      "Epoch 13/200\n",
      "230/230 - 1s - loss: 0.0025 - val_loss: 0.0036 - 1s/epoch - 5ms/step\n",
      "Epoch 14/200\n",
      "230/230 - 1s - loss: 0.0023 - val_loss: 0.0036 - 1s/epoch - 5ms/step\n",
      "Epoch 15/200\n",
      "230/230 - 1s - loss: 0.0022 - val_loss: 0.0037 - 1s/epoch - 5ms/step\n",
      "Epoch 16/200\n",
      "230/230 - 1s - loss: 0.0022 - val_loss: 0.0034 - 1s/epoch - 5ms/step\n",
      "Epoch 17/200\n",
      "230/230 - 1s - loss: 0.0021 - val_loss: 0.0034 - 1s/epoch - 5ms/step\n",
      "Epoch 18/200\n",
      "230/230 - 1s - loss: 0.0021 - val_loss: 0.0032 - 1s/epoch - 5ms/step\n",
      "Epoch 19/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0031 - 1s/epoch - 5ms/step\n",
      "Epoch 20/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0029 - 1s/epoch - 5ms/step\n",
      "Epoch 21/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0029 - 1s/epoch - 5ms/step\n",
      "Epoch 22/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0031 - 1s/epoch - 5ms/step\n",
      "Epoch 23/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0028 - 1s/epoch - 5ms/step\n",
      "Epoch 24/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 25/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 26/200\n",
      "230/230 - 1s - loss: 0.0017 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 27/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 28/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 29/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0027 - 1s/epoch - 5ms/step\n",
      "Epoch 30/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 31/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 32/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 33/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 34/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 35/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 36/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 37/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 38/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 39/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 40/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 41/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 42/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 43/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 44/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 45/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 46/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 47/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 48/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 49/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 50/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 51/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 52/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 53/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 54/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 55/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "230/230 - 1s - loss: 9.5443e-04 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 57/200\n",
      "230/230 - 1s - loss: 8.6000e-04 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 58/200\n",
      "230/230 - 1s - loss: 8.5800e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 59/200\n",
      "230/230 - 1s - loss: 8.3912e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 60/200\n",
      "230/230 - 1s - loss: 8.3063e-04 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 61/200\n",
      "230/230 - 1s - loss: 8.3301e-04 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 62/200\n",
      "230/230 - 1s - loss: 8.3191e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 63/200\n",
      "230/230 - 1s - loss: 8.2164e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 64/200\n",
      "230/230 - 1s - loss: 7.9839e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 65/200\n",
      "230/230 - 1s - loss: 7.9315e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 66/200\n",
      "230/230 - 1s - loss: 7.7936e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 67/200\n",
      "230/230 - 1s - loss: 7.7657e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 68/200\n",
      "230/230 - 1s - loss: 7.6435e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 69/200\n",
      "230/230 - 1s - loss: 7.6375e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 70/200\n",
      "230/230 - 1s - loss: 7.2774e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 71/200\n",
      "230/230 - 1s - loss: 7.3460e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 72/200\n",
      "230/230 - 1s - loss: 7.0791e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 73/200\n",
      "230/230 - 1s - loss: 6.8577e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 74/200\n",
      "230/230 - 1s - loss: 6.7737e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "230/230 - 1s - loss: 6.7909e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 76/200\n",
      "230/230 - 1s - loss: 6.6929e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 77/200\n",
      "230/230 - 1s - loss: 6.5194e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 78/200\n",
      "230/230 - 1s - loss: 6.4517e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 79/200\n",
      "230/230 - 1s - loss: 6.4712e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 80/200\n",
      "230/230 - 1s - loss: 6.4005e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 81/200\n",
      "230/230 - 1s - loss: 6.2986e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 82/200\n",
      "230/230 - 1s - loss: 6.2274e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 83/200\n",
      "230/230 - 1s - loss: 6.2782e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 84/200\n",
      "230/230 - 1s - loss: 5.8695e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 85/200\n",
      "230/230 - 1s - loss: 5.7691e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 86/200\n",
      "230/230 - 1s - loss: 5.7568e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 87/200\n",
      "230/230 - 1s - loss: 5.6327e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 88/200\n",
      "230/230 - 1s - loss: 5.4303e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 89/200\n",
      "230/230 - 1s - loss: 5.3602e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 90/200\n",
      "230/230 - 1s - loss: 5.2621e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 91/200\n",
      "230/230 - 1s - loss: 5.3403e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 92/200\n",
      "230/230 - 1s - loss: 5.2590e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 93/200\n",
      "230/230 - 1s - loss: 5.2707e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 94/200\n",
      "230/230 - 1s - loss: 5.1248e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 95/200\n",
      "230/230 - 1s - loss: 5.2966e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 96/200\n",
      "230/230 - 1s - loss: 5.0871e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 97/200\n",
      "230/230 - 1s - loss: 5.0122e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 98/200\n",
      "230/230 - 1s - loss: 5.0265e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 99/200\n",
      "230/230 - 1s - loss: 4.9833e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 100/200\n",
      "230/230 - 1s - loss: 4.9603e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 101/200\n",
      "230/230 - 1s - loss: 4.9075e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 102/200\n",
      "230/230 - 1s - loss: 4.9267e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 103/200\n",
      "230/230 - 1s - loss: 4.8477e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 104/200\n",
      "230/230 - 1s - loss: 4.8233e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 105/200\n",
      "230/230 - 1s - loss: 4.7951e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 106/200\n",
      "230/230 - 1s - loss: 4.9077e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 107/200\n",
      "230/230 - 1s - loss: 4.7250e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 108/200\n",
      "230/230 - 1s - loss: 4.6553e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 109/200\n",
      "230/230 - 1s - loss: 4.6137e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 110/200\n",
      "230/230 - 1s - loss: 4.6151e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 111/200\n",
      "230/230 - 1s - loss: 4.5917e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 112/200\n",
      "230/230 - 1s - loss: 4.4579e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 113/200\n",
      "230/230 - 1s - loss: 4.4937e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 114/200\n",
      "230/230 - 1s - loss: 4.5247e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 115/200\n",
      "230/230 - 1s - loss: 4.4642e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 116/200\n",
      "230/230 - 1s - loss: 4.3952e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 117/200\n",
      "230/230 - 1s - loss: 4.4251e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 118/200\n",
      "230/230 - 1s - loss: 4.3059e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 119/200\n",
      "230/230 - 1s - loss: 4.3581e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 120/200\n",
      "230/230 - 1s - loss: 4.3532e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 121/200\n",
      "230/230 - 1s - loss: 4.2961e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 122/200\n",
      "230/230 - 1s - loss: 4.2632e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 123/200\n",
      "230/230 - 1s - loss: 4.2414e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 124/200\n",
      "230/230 - 1s - loss: 4.1958e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 125/200\n",
      "230/230 - 1s - loss: 4.2018e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 126/200\n",
      "230/230 - 1s - loss: 4.2105e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 127/200\n",
      "230/230 - 1s - loss: 4.1130e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 128/200\n",
      "230/230 - 1s - loss: 4.0506e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 129/200\n",
      "230/230 - 1s - loss: 4.1129e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 130/200\n",
      "230/230 - 1s - loss: 4.0695e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 131/200\n",
      "230/230 - 1s - loss: 4.0695e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 132/200\n",
      "230/230 - 1s - loss: 4.0118e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 133/200\n",
      "230/230 - 1s - loss: 3.9664e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 134/200\n",
      "230/230 - 1s - loss: 3.9514e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 135/200\n",
      "230/230 - 1s - loss: 3.9839e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 136/200\n",
      "230/230 - 1s - loss: 4.0483e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 137/200\n",
      "230/230 - 1s - loss: 3.8724e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 138/200\n",
      "230/230 - 1s - loss: 3.8393e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 139/200\n",
      "230/230 - 1s - loss: 3.8363e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 140/200\n",
      "230/230 - 1s - loss: 3.9421e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 141/200\n",
      "230/230 - 1s - loss: 3.8668e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 142/200\n",
      "230/230 - 1s - loss: 3.9065e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 143/200\n",
      "230/230 - 1s - loss: 3.7423e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 144/200\n",
      "230/230 - 1s - loss: 3.8045e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 145/200\n",
      "230/230 - 1s - loss: 3.7565e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 146/200\n",
      "230/230 - 1s - loss: 3.7243e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 147/200\n",
      "230/230 - 1s - loss: 3.6811e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 148/200\n",
      "230/230 - 1s - loss: 3.6954e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 149/200\n",
      "230/230 - 1s - loss: 3.8018e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 150/200\n",
      "230/230 - 1s - loss: 3.7023e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "230/230 - 1s - loss: 3.6679e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 152/200\n",
      "230/230 - 1s - loss: 3.6645e-04 - val_loss: 9.7932e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 153/200\n",
      "230/230 - 1s - loss: 3.6712e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 154/200\n",
      "230/230 - 1s - loss: 3.6136e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 155/200\n",
      "230/230 - 1s - loss: 3.5986e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 156/200\n",
      "230/230 - 1s - loss: 3.6072e-04 - val_loss: 9.9316e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 157/200\n",
      "230/230 - 1s - loss: 3.5189e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 158/200\n",
      "230/230 - 1s - loss: 3.6461e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 159/200\n",
      "230/230 - 1s - loss: 3.5732e-04 - val_loss: 9.6903e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 160/200\n",
      "230/230 - 1s - loss: 3.8599e-04 - val_loss: 9.8740e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 161/200\n",
      "230/230 - 1s - loss: 3.4384e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 162/200\n",
      "230/230 - 1s - loss: 3.5197e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 163/200\n",
      "230/230 - 1s - loss: 3.4427e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 164/200\n",
      "230/230 - 1s - loss: 3.4804e-04 - val_loss: 9.5795e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 165/200\n",
      "230/230 - 1s - loss: 3.4619e-04 - val_loss: 9.7412e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 166/200\n",
      "230/230 - 1s - loss: 3.4587e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 167/200\n",
      "230/230 - 1s - loss: 7.3215e-04 - val_loss: 9.7431e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 168/200\n",
      "230/230 - 1s - loss: 3.4286e-04 - val_loss: 9.7992e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 169/200\n",
      "230/230 - 1s - loss: 3.3121e-04 - val_loss: 9.4029e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 170/200\n",
      "230/230 - 1s - loss: 3.2883e-04 - val_loss: 9.5344e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 171/200\n",
      "230/230 - 1s - loss: 3.3053e-04 - val_loss: 9.5727e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 172/200\n",
      "230/230 - 1s - loss: 3.3113e-04 - val_loss: 9.4242e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 173/200\n",
      "230/230 - 1s - loss: 3.2820e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 174/200\n",
      "230/230 - 1s - loss: 3.3508e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 175/200\n",
      "230/230 - 1s - loss: 3.3282e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 176/200\n",
      "230/230 - 1s - loss: 3.2888e-04 - val_loss: 9.6032e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 177/200\n",
      "230/230 - 1s - loss: 3.3010e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 178/200\n",
      "230/230 - 1s - loss: 3.2878e-04 - val_loss: 9.5503e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 179/200\n",
      "230/230 - 1s - loss: 3.2954e-04 - val_loss: 9.4404e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 180/200\n",
      "230/230 - 1s - loss: 3.2719e-04 - val_loss: 9.6704e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 181/200\n",
      "230/230 - 1s - loss: 3.2374e-04 - val_loss: 9.2572e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 182/200\n",
      "230/230 - 1s - loss: 3.2601e-04 - val_loss: 9.6524e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 183/200\n",
      "230/230 - 1s - loss: 3.2059e-04 - val_loss: 9.8043e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 184/200\n",
      "230/230 - 1s - loss: 3.2002e-04 - val_loss: 9.2765e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 185/200\n",
      "230/230 - 1s - loss: 3.1678e-04 - val_loss: 9.7655e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 186/200\n",
      "230/230 - 1s - loss: 3.2704e-04 - val_loss: 9.7426e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 187/200\n",
      "230/230 - 1s - loss: 3.1259e-04 - val_loss: 9.1928e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 188/200\n",
      "230/230 - 1s - loss: 3.1814e-04 - val_loss: 9.2464e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 189/200\n",
      "230/230 - 1s - loss: 3.1940e-04 - val_loss: 9.1897e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 190/200\n",
      "230/230 - 1s - loss: 3.1626e-04 - val_loss: 9.6369e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 191/200\n",
      "230/230 - 1s - loss: 3.1792e-04 - val_loss: 9.3595e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 192/200\n",
      "230/230 - 1s - loss: 3.1599e-04 - val_loss: 9.2920e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 193/200\n",
      "230/230 - 1s - loss: 3.1252e-04 - val_loss: 9.3181e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 194/200\n",
      "230/230 - 1s - loss: 3.1093e-04 - val_loss: 9.1467e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 195/200\n",
      "230/230 - 1s - loss: 3.1692e-04 - val_loss: 9.3331e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 196/200\n",
      "230/230 - 1s - loss: 3.0997e-04 - val_loss: 9.2444e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 197/200\n",
      "230/230 - 1s - loss: 3.1133e-04 - val_loss: 9.0769e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 198/200\n",
      "230/230 - 1s - loss: 3.2104e-04 - val_loss: 9.2937e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 199/200\n",
      "230/230 - 1s - loss: 3.0202e-04 - val_loss: 9.1653e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 200/200\n",
      "230/230 - 1s - loss: 3.0418e-04 - val_loss: 9.3011e-04 - 1s/epoch - 5ms/step\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1, 137)]          0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1, 64)             26368     \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 1, 32)             12416     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 1, 32)             8320      \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 1, 32)             8320      \n",
      "                                                                 \n",
      " conv1d_transpose_4 (Conv1DT  (None, 1, 64)            6208      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_5 (Conv1DT  (None, 1, 137)           26441     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,073\n",
      "Trainable params: 88,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "230/230 - 4s - loss: 0.0693 - val_loss: 0.0215 - 4s/epoch - 17ms/step\n",
      "Epoch 2/200\n",
      "230/230 - 1s - loss: 0.0148 - val_loss: 0.0142 - 1s/epoch - 5ms/step\n",
      "Epoch 3/200\n",
      "230/230 - 1s - loss: 0.0089 - val_loss: 0.0115 - 1s/epoch - 5ms/step\n",
      "Epoch 4/200\n",
      "230/230 - 1s - loss: 0.0066 - val_loss: 0.0104 - 1s/epoch - 5ms/step\n",
      "Epoch 5/200\n",
      "230/230 - 1s - loss: 0.0058 - val_loss: 0.0076 - 1s/epoch - 5ms/step\n",
      "Epoch 6/200\n",
      "230/230 - 1s - loss: 0.0053 - val_loss: 0.0073 - 1s/epoch - 5ms/step\n",
      "Epoch 7/200\n",
      "230/230 - 1s - loss: 0.0049 - val_loss: 0.0067 - 1s/epoch - 5ms/step\n",
      "Epoch 8/200\n",
      "230/230 - 1s - loss: 0.0042 - val_loss: 0.0055 - 1s/epoch - 5ms/step\n",
      "Epoch 9/200\n",
      "230/230 - 1s - loss: 0.0037 - val_loss: 0.0051 - 1s/epoch - 5ms/step\n",
      "Epoch 10/200\n",
      "230/230 - 1s - loss: 0.0033 - val_loss: 0.0044 - 1s/epoch - 5ms/step\n",
      "Epoch 11/200\n",
      "230/230 - 1s - loss: 0.0031 - val_loss: 0.0040 - 1s/epoch - 5ms/step\n",
      "Epoch 12/200\n",
      "230/230 - 1s - loss: 0.0029 - val_loss: 0.0039 - 1s/epoch - 5ms/step\n",
      "Epoch 13/200\n",
      "230/230 - 1s - loss: 0.0028 - val_loss: 0.0044 - 1s/epoch - 5ms/step\n",
      "Epoch 14/200\n",
      "230/230 - 1s - loss: 0.0027 - val_loss: 0.0035 - 1s/epoch - 5ms/step\n",
      "Epoch 15/200\n",
      "230/230 - 1s - loss: 0.0026 - val_loss: 0.0037 - 1s/epoch - 5ms/step\n",
      "Epoch 16/200\n",
      "230/230 - 1s - loss: 0.0023 - val_loss: 0.0033 - 1s/epoch - 5ms/step\n",
      "Epoch 17/200\n",
      "230/230 - 1s - loss: 0.0022 - val_loss: 0.0033 - 1s/epoch - 5ms/step\n",
      "Epoch 18/200\n",
      "230/230 - 1s - loss: 0.0022 - val_loss: 0.0028 - 1s/epoch - 5ms/step\n",
      "Epoch 19/200\n",
      "230/230 - 1s - loss: 0.0021 - val_loss: 0.0033 - 1s/epoch - 5ms/step\n",
      "Epoch 20/200\n",
      "230/230 - 1s - loss: 0.0021 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 21/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0030 - 1s/epoch - 5ms/step\n",
      "Epoch 22/200\n",
      "230/230 - 1s - loss: 0.0020 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 23/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 24/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0026 - 1s/epoch - 5ms/step\n",
      "Epoch 25/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0040 - 1s/epoch - 5ms/step\n",
      "Epoch 26/200\n",
      "230/230 - 1s - loss: 0.0019 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 27/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0024 - 1s/epoch - 5ms/step\n",
      "Epoch 28/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "230/230 - 1s - loss: 0.0018 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 30/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0023 - 1s/epoch - 5ms/step\n",
      "Epoch 31/200\n",
      "230/230 - 1s - loss: 0.0016 - val_loss: 0.0022 - 1s/epoch - 5ms/step\n",
      "Epoch 32/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 33/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0020 - 1s/epoch - 5ms/step\n",
      "Epoch 34/200\n",
      "230/230 - 1s - loss: 0.0015 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 35/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0021 - 1s/epoch - 5ms/step\n",
      "Epoch 36/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0025 - 1s/epoch - 5ms/step\n",
      "Epoch 37/200\n",
      "230/230 - 1s - loss: 0.0014 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 38/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 39/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 40/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0018 - 1s/epoch - 5ms/step\n",
      "Epoch 41/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 42/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0019 - 1s/epoch - 5ms/step\n",
      "Epoch 43/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0031 - 1s/epoch - 5ms/step\n",
      "Epoch 44/200\n",
      "230/230 - 1s - loss: 0.0013 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 45/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 46/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 47/200\n",
      "230/230 - 1s - loss: 0.0012 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 48/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 49/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 50/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 51/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 52/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 53/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 54/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 55/200\n",
      "230/230 - 1s - loss: 0.0011 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 56/200\n",
      "230/230 - 1s - loss: 0.0010 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 57/200\n",
      "230/230 - 1s - loss: 0.0010 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 58/200\n",
      "230/230 - 1s - loss: 0.0010 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 59/200\n",
      "230/230 - 1s - loss: 9.9478e-04 - val_loss: 0.0016 - 1s/epoch - 5ms/step\n",
      "Epoch 60/200\n",
      "230/230 - 1s - loss: 0.0010 - val_loss: 0.0015 - 1s/epoch - 6ms/step\n",
      "Epoch 61/200\n",
      "230/230 - 1s - loss: 9.8014e-04 - val_loss: 0.0014 - 1s/epoch - 6ms/step\n",
      "Epoch 62/200\n",
      "230/230 - 1s - loss: 9.7582e-04 - val_loss: 0.0014 - 1s/epoch - 6ms/step\n",
      "Epoch 63/200\n",
      "230/230 - 1s - loss: 9.6647e-04 - val_loss: 0.0014 - 1s/epoch - 6ms/step\n",
      "Epoch 64/200\n",
      "230/230 - 1s - loss: 9.6524e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 65/200\n",
      "230/230 - 1s - loss: 9.6047e-04 - val_loss: 0.0015 - 1s/epoch - 5ms/step\n",
      "Epoch 66/200\n",
      "230/230 - 1s - loss: 9.5488e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 67/200\n",
      "230/230 - 1s - loss: 9.5006e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 68/200\n",
      "230/230 - 1s - loss: 9.4172e-04 - val_loss: 0.0017 - 1s/epoch - 5ms/step\n",
      "Epoch 69/200\n",
      "230/230 - 1s - loss: 9.5795e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 70/200\n",
      "230/230 - 1s - loss: 9.1126e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 71/200\n",
      "230/230 - 1s - loss: 9.1702e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 72/200\n",
      "230/230 - 1s - loss: 9.1162e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 73/200\n",
      "230/230 - 1s - loss: 9.0591e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 74/200\n",
      "230/230 - 1s - loss: 8.9880e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 75/200\n",
      "230/230 - 1s - loss: 8.9312e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 76/200\n",
      "230/230 - 1s - loss: 8.7499e-04 - val_loss: 0.0013 - 1s/epoch - 6ms/step\n",
      "Epoch 77/200\n",
      "230/230 - 1s - loss: 8.7391e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 78/200\n",
      "230/230 - 1s - loss: 8.7205e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 79/200\n",
      "230/230 - 1s - loss: 8.6031e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 80/200\n",
      "230/230 - 1s - loss: 8.5753e-04 - val_loss: 0.0014 - 1s/epoch - 6ms/step\n",
      "Epoch 81/200\n",
      "230/230 - 1s - loss: 8.5323e-04 - val_loss: 0.0013 - 1s/epoch - 6ms/step\n",
      "Epoch 82/200\n",
      "230/230 - 1s - loss: 8.5187e-04 - val_loss: 0.0013 - 1s/epoch - 6ms/step\n",
      "Epoch 83/200\n",
      "230/230 - 1s - loss: 8.3428e-04 - val_loss: 0.0013 - 1s/epoch - 6ms/step\n",
      "Epoch 84/200\n",
      "230/230 - 1s - loss: 8.2791e-04 - val_loss: 0.0018 - 1s/epoch - 6ms/step\n",
      "Epoch 85/200\n",
      "230/230 - 1s - loss: 8.4369e-04 - val_loss: 0.0015 - 1s/epoch - 6ms/step\n",
      "Epoch 86/200\n",
      "230/230 - 1s - loss: 8.3409e-04 - val_loss: 0.0012 - 1s/epoch - 6ms/step\n",
      "Epoch 87/200\n",
      "230/230 - 1s - loss: 8.1280e-04 - val_loss: 0.0012 - 1s/epoch - 6ms/step\n",
      "Epoch 88/200\n",
      "230/230 - 1s - loss: 8.1573e-04 - val_loss: 0.0012 - 1s/epoch - 6ms/step\n",
      "Epoch 89/200\n",
      "230/230 - 1s - loss: 8.0571e-04 - val_loss: 0.0014 - 1s/epoch - 5ms/step\n",
      "Epoch 90/200\n",
      "230/230 - 1s - loss: 8.0617e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 91/200\n",
      "230/230 - 1s - loss: 7.9758e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 92/200\n",
      "230/230 - 1s - loss: 7.9496e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 93/200\n",
      "230/230 - 1s - loss: 7.9109e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 94/200\n",
      "230/230 - 1s - loss: 7.9214e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 95/200\n",
      "230/230 - 1s - loss: 7.8324e-04 - val_loss: 0.0013 - 1s/epoch - 5ms/step\n",
      "Epoch 96/200\n",
      "230/230 - 1s - loss: 7.9431e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 97/200\n",
      "230/230 - 1s - loss: 7.7585e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 98/200\n",
      "230/230 - 1s - loss: 7.8183e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 99/200\n",
      "230/230 - 1s - loss: 7.7303e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 100/200\n",
      "230/230 - 1s - loss: 7.5700e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 101/200\n",
      "230/230 - 1s - loss: 7.6460e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 102/200\n",
      "230/230 - 1s - loss: 7.6328e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 103/200\n",
      "230/230 - 1s - loss: 7.4951e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 104/200\n",
      "230/230 - 1s - loss: 7.4947e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 105/200\n",
      "230/230 - 1s - loss: 7.5782e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 106/200\n",
      "230/230 - 1s - loss: 7.3951e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 107/200\n",
      "230/230 - 1s - loss: 7.5539e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 108/200\n",
      "230/230 - 1s - loss: 7.4194e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 109/200\n",
      "230/230 - 1s - loss: 7.3480e-04 - val_loss: 0.0012 - 1s/epoch - 6ms/step\n",
      "Epoch 110/200\n",
      "230/230 - 1s - loss: 7.4680e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 111/200\n",
      "230/230 - 1s - loss: 7.2876e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 112/200\n",
      "230/230 - 1s - loss: 7.2509e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 113/200\n",
      "230/230 - 1s - loss: 7.2529e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n",
      "Epoch 114/200\n",
      "230/230 - 1s - loss: 7.2630e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n",
      "Epoch 115/200\n",
      "230/230 - 1s - loss: 7.1244e-04 - val_loss: 0.0012 - 1s/epoch - 6ms/step\n",
      "Epoch 116/200\n",
      "230/230 - 1s - loss: 7.0758e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n",
      "Epoch 117/200\n",
      "230/230 - 1s - loss: 7.0513e-04 - val_loss: 0.0012 - 1s/epoch - 6ms/step\n",
      "Epoch 118/200\n",
      "230/230 - 1s - loss: 7.0998e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 119/200\n",
      "230/230 - 1s - loss: 7.0992e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 120/200\n",
      "230/230 - 1s - loss: 6.8783e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 121/200\n",
      "230/230 - 1s - loss: 6.9190e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 122/200\n",
      "230/230 - 1s - loss: 6.8282e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 123/200\n",
      "230/230 - 1s - loss: 6.9161e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 124/200\n",
      "230/230 - 1s - loss: 6.8250e-04 - val_loss: 0.0012 - 1s/epoch - 5ms/step\n",
      "Epoch 125/200\n",
      "230/230 - 1s - loss: 6.8232e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200\n",
      "230/230 - 1s - loss: 6.9466e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 127/200\n",
      "230/230 - 1s - loss: 6.7816e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 128/200\n",
      "230/230 - 1s - loss: 6.7200e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 129/200\n",
      "230/230 - 1s - loss: 6.7931e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n",
      "Epoch 130/200\n",
      "230/230 - 1s - loss: 6.7839e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 131/200\n",
      "230/230 - 1s - loss: 6.6776e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 132/200\n",
      "230/230 - 1s - loss: 6.7461e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 133/200\n",
      "230/230 - 1s - loss: 6.6088e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 134/200\n",
      "230/230 - 1s - loss: 6.6374e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 135/200\n",
      "230/230 - 1s - loss: 6.6291e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 136/200\n",
      "230/230 - 1s - loss: 6.6393e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 137/200\n",
      "230/230 - 1s - loss: 6.6683e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 138/200\n",
      "230/230 - 1s - loss: 6.5248e-04 - val_loss: 9.8447e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 139/200\n",
      "230/230 - 1s - loss: 6.4910e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 140/200\n",
      "230/230 - 1s - loss: 6.5378e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 141/200\n",
      "230/230 - 1s - loss: 6.5392e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 142/200\n",
      "230/230 - 1s - loss: 6.4788e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 143/200\n",
      "230/230 - 1s - loss: 6.4807e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 144/200\n",
      "230/230 - 1s - loss: 6.5131e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 145/200\n",
      "230/230 - 1s - loss: 6.4372e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 146/200\n",
      "230/230 - 1s - loss: 6.5341e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 147/200\n",
      "230/230 - 1s - loss: 6.4704e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 148/200\n",
      "230/230 - 1s - loss: 6.3779e-04 - val_loss: 9.9690e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 149/200\n",
      "230/230 - 1s - loss: 6.3918e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 150/200\n",
      "230/230 - 1s - loss: 6.3045e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 151/200\n",
      "230/230 - 1s - loss: 6.4283e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 152/200\n",
      "230/230 - 1s - loss: 6.3081e-04 - val_loss: 9.9948e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 153/200\n",
      "230/230 - 1s - loss: 6.3633e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 154/200\n",
      "230/230 - 1s - loss: 6.2760e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 155/200\n",
      "230/230 - 1s - loss: 6.2196e-04 - val_loss: 9.9725e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 156/200\n",
      "230/230 - 1s - loss: 6.2531e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 157/200\n",
      "230/230 - 1s - loss: 6.2666e-04 - val_loss: 0.0010 - 1s/epoch - 6ms/step\n",
      "Epoch 158/200\n",
      "230/230 - 1s - loss: 6.1642e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 159/200\n",
      "230/230 - 1s - loss: 6.1518e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n",
      "Epoch 160/200\n",
      "230/230 - 1s - loss: 6.0795e-04 - val_loss: 0.0010 - 1s/epoch - 6ms/step\n",
      "Epoch 161/200\n",
      "230/230 - 1s - loss: 6.0782e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 162/200\n",
      "230/230 - 1s - loss: 6.0736e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 163/200\n",
      "230/230 - 1s - loss: 6.0750e-04 - val_loss: 9.7122e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 164/200\n",
      "230/230 - 1s - loss: 6.0442e-04 - val_loss: 9.8347e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 165/200\n",
      "230/230 - 1s - loss: 6.0081e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 166/200\n",
      "230/230 - 1s - loss: 6.0447e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 167/200\n",
      "230/230 - 1s - loss: 5.9526e-04 - val_loss: 9.7651e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 168/200\n",
      "230/230 - 1s - loss: 5.9782e-04 - val_loss: 9.6216e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 169/200\n",
      "230/230 - 1s - loss: 5.9425e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 170/200\n",
      "230/230 - 1s - loss: 5.9851e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 171/200\n",
      "230/230 - 1s - loss: 5.9357e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 172/200\n",
      "230/230 - 1s - loss: 5.9251e-04 - val_loss: 9.5188e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 173/200\n",
      "230/230 - 1s - loss: 5.9128e-04 - val_loss: 9.5628e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 174/200\n",
      "230/230 - 1s - loss: 5.9882e-04 - val_loss: 9.9944e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 175/200\n",
      "230/230 - 1s - loss: 5.8779e-04 - val_loss: 9.7748e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 176/200\n",
      "230/230 - 1s - loss: 5.8508e-04 - val_loss: 9.3972e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 177/200\n",
      "230/230 - 1s - loss: 5.8644e-04 - val_loss: 9.9703e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 178/200\n",
      "230/230 - 1s - loss: 5.8206e-04 - val_loss: 9.6643e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 179/200\n",
      "230/230 - 1s - loss: 5.8900e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 180/200\n",
      "230/230 - 1s - loss: 5.8767e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 181/200\n",
      "230/230 - 1s - loss: 5.8644e-04 - val_loss: 9.4921e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 182/200\n",
      "230/230 - 1s - loss: 5.8215e-04 - val_loss: 9.3545e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 183/200\n",
      "230/230 - 1s - loss: 5.7888e-04 - val_loss: 9.4966e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 184/200\n",
      "230/230 - 1s - loss: 5.7772e-04 - val_loss: 9.5661e-04 - 1s/epoch - 5ms/step\n",
      "Epoch 185/200\n",
      "230/230 - 1s - loss: 5.8480e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 186/200\n",
      "230/230 - 1s - loss: 5.7963e-04 - val_loss: 0.0010 - 1s/epoch - 5ms/step\n",
      "Epoch 187/200\n",
      "230/230 - 1s - loss: 5.7879e-04 - val_loss: 0.0011 - 1s/epoch - 5ms/step\n",
      "Epoch 188/200\n",
      "230/230 - 1s - loss: 5.9103e-04 - val_loss: 9.3072e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 189/200\n",
      "230/230 - 1s - loss: 5.7208e-04 - val_loss: 9.5787e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 190/200\n",
      "230/230 - 1s - loss: 5.6936e-04 - val_loss: 9.3392e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 191/200\n",
      "230/230 - 1s - loss: 5.7693e-04 - val_loss: 9.5987e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 192/200\n",
      "230/230 - 1s - loss: 5.7131e-04 - val_loss: 0.0011 - 1s/epoch - 6ms/step\n",
      "Epoch 193/200\n",
      "230/230 - 1s - loss: 7.8903e-04 - val_loss: 9.1363e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 194/200\n",
      "230/230 - 1s - loss: 5.5933e-04 - val_loss: 9.3098e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 195/200\n",
      "230/230 - 1s - loss: 5.5924e-04 - val_loss: 9.0070e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 196/200\n",
      "230/230 - 1s - loss: 5.5702e-04 - val_loss: 9.4580e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 197/200\n",
      "230/230 - 1s - loss: 5.5708e-04 - val_loss: 9.5741e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 198/200\n",
      "230/230 - 2s - loss: 5.5979e-04 - val_loss: 9.5747e-04 - 2s/epoch - 7ms/step\n",
      "Epoch 199/200\n",
      "230/230 - 1s - loss: 5.6532e-04 - val_loss: 9.4882e-04 - 1s/epoch - 6ms/step\n",
      "Epoch 200/200\n",
      "230/230 - 1s - loss: 5.5682e-04 - val_loss: 9.2002e-04 - 1s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def Conv_LstmAEModel(timesteps = 1, n_features=137, layer1=100, layer2=60, encoding_dim=4):\n",
    "    input = Input(shape=(timesteps, n_features))\n",
    "    x = Conv1D(filters=layer1, activation=\"relu\",kernel_size=3, padding=\"same\")(input)\n",
    "    x = LSTM(layer2,  activation=\"relu\", return_sequences=True)(x)\n",
    "    x = LSTM(encoding_dim, activation=\"relu\", return_sequences=True)(x)\n",
    "    x = LSTM(layer2, activation=\"relu\", return_sequences=True)(x)\n",
    "    x = Conv1DTranspose(filters=layer1,  activation=\"relu\",kernel_size=3, padding=\"same\")(x)\n",
    "    x = Conv1DTranspose(filters=n_features,  activation=\"sigmoid\",kernel_size=3, padding=\"same\")(x)\n",
    "    autoencoder = Model(input, x)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    autoencoder.summary()\n",
    "    return autoencoder\n",
    "def Conv_LstmAttnAEModel(timesteps = 1, n_features=137, layer1=100, layer2=60, encoding_dim=4):\n",
    "    input = Input(shape=(timesteps, n_features))\n",
    "    x = Conv1D(filters=layer1, activation=\"relu\",kernel_size=3, padding=\"same\")(input)\n",
    "    x = LSTM(layer2,  activation=\"relu\", return_sequences=True)(x)\n",
    "    x = LSTM(encoding_dim, activation=\"relu\", return_sequences=True)(x)\n",
    "    attention = Attention()([x, x])\n",
    "    attention = GlobalMaxPooling1D()(attention)\n",
    "    x = Reshape((1, encoding_dim))(attention)\n",
    "    x = LSTM(layer2, activation=\"relu\", return_sequences=True)(x)\n",
    "    x = Conv1DTranspose(filters=layer1,  activation=\"relu\",kernel_size=3, padding=\"same\")(x)\n",
    "    x = Conv1DTranspose(filters=n_features,  activation=\"sigmoid\",kernel_size=3, padding=\"same\")(x)\n",
    "    autoencoder = Model(input, x)\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "#     autoencoder.summary()\n",
    "    return autoencoder\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2'] \n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "    \n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2])) \n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Conv_LstmAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = xtrain.reshape(xtrain.shape[0], 1, xtrain.shape[1])\n",
    "        xtest = xtest.reshape(xtest.shape[0], 1, xtest.shape[1])\n",
    "#         ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)                                               \n",
    "        model  = Conv_LstmAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_Conv_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_Conv_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = train_merge_df.reshape(train_merge_df.shape[0], 1, train_merge_df.shape[1])\n",
    "\n",
    "    model = Conv_LstmAEModel(layer1=64, layer2=32, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = xtrain.reshape(xtrain.shape[0], 1, xtrain.shape[1])\n",
    "        xtest = xtest.reshape(xtest.shape[0], 1, xtest.shape[1])\n",
    "#         ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_Conv_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_Conv_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVLSTM-ATTNAE     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Conv_LstmAttnAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = xtrain.reshape(xtrain.shape[0], 1, xtrain.shape[1])\n",
    "        xtest = xtest.reshape(xtest.shape[0], 1, xtest.shape[1])\n",
    "#         ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)                                               \n",
    "        model  = Conv_LstmAttnAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_Conv_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_Conv_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = train_merge_df.reshape(train_merge_df.shape[0], 1, train_merge_df.shape[1])\n",
    "\n",
    "    model = Conv_LstmAttnAEModel(layer1=64, layer2=32, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = xtrain.reshape(xtrain.shape[0], 1, xtrain.shape[1])\n",
    "        xtest = xtest.reshape(xtest.shape[0], 1, xtest.shape[1])\n",
    "#         ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_Conv_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_Conv_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "799209b20b2b97cb7d521719e25d3ce7f86c9d675fdfb18aaaa1ffc1f301c415"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
