{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd88a301",
   "metadata": {},
   "source": [
    "## TCNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bf23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def TCNAEModel(lookback=8, ts_dims = 137, layer1=20, layer2=6, encoding_dim=4):\n",
    "    model = Sequential([\n",
    "    TCN(input_shape=(lookback, ts_dims), nb_filters=layer1, kernel_size=3, padding='same',activation='relu', return_sequences=True),\n",
    "    Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same'),\n",
    "    Conv1D(filters=encoding_dim, kernel_size=3, activation='relu', padding='same'),\n",
    "    AveragePooling1D(pool_size=4, strides=None, padding='valid'),\n",
    "    Activation(\"linear\"),\n",
    "    UpSampling1D(size=4),\n",
    "    Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same'),\n",
    "    TCN(nb_filters=layer1, kernel_size=3, padding='same',activation='relu',return_sequences=True),\n",
    "    Dense(ts_dims, activation='sigmoid')\n",
    "])\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2']\n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "\n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/TCNAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)\n",
    "        model  = TCNAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = temporalize(train_merge_df)\n",
    "\n",
    "    model = TCNAEModel(layer1=128, layer2=64, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_TcnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca98dc",
   "metadata": {},
   "source": [
    "## TCNATTNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def TCNAttnAEModel(lookback=8, n_features=137, layer1=100, layer2=64, encoding_dim = 4):\n",
    "    encoder_inputs = Input(shape=(8, n_features))\n",
    "    encoder = TCN(nb_filters=layer1, kernel_size=3, padding='same',activation='relu', return_sequences=True)(encoder_inputs)\n",
    "    encoder = Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "    encoder = Conv1D(filters=encoding_dim, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "    attention = Attention()([encoder, encoder])\n",
    "    attention = AveragePooling1D(pool_size=4, strides=None, padding='valid')(attention)\n",
    "    decoder = UpSampling1D(size=4)(attention)\n",
    "    decoder = Conv1D(filters=layer2, kernel_size=3, activation='relu', padding='same')(decoder)\n",
    "    decoder = TCN(nb_filters=layer1, kernel_size=3, padding='same',activation='relu',return_sequences=True)(decoder)\n",
    "    decoder_outputs = Dense(n_features, activation='sigmoid')(decoder)\n",
    "    autoencoder = Model(encoder_inputs, decoder_outputs)\n",
    "    autoencoder.compile(loss='mse', optimizer='adam')    \n",
    "    return autoencoder\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2'] \n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "    \n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2])) \n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/TCNATTNAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)                                               \n",
    "        model  = TCNAttnAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = temporalize(train_merge_df)\n",
    "\n",
    "    model = TCNAttnAEModel(layer1=128, layer2=64, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_TcnAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72fe62",
   "metadata": {},
   "source": [
    "## LSTMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cac106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "109/109 - 12s - loss: 0.0493 - val_loss: 0.0558 - 12s/epoch - 111ms/step\n",
      "Epoch 2/100\n",
      "109/109 - 4s - loss: 0.0237 - val_loss: 0.0538 - 4s/epoch - 36ms/step\n",
      "Epoch 3/100\n",
      "109/109 - 4s - loss: 0.0228 - val_loss: 0.0528 - 4s/epoch - 38ms/step\n",
      "Epoch 4/100\n",
      "109/109 - 4s - loss: 0.0223 - val_loss: 0.0519 - 4s/epoch - 37ms/step\n",
      "Epoch 5/100\n",
      "109/109 - 4s - loss: 0.0219 - val_loss: 0.0514 - 4s/epoch - 38ms/step\n",
      "Epoch 6/100\n",
      "109/109 - 4s - loss: 0.0217 - val_loss: 0.0504 - 4s/epoch - 37ms/step\n",
      "Epoch 7/100\n",
      "109/109 - 4s - loss: 0.0215 - val_loss: 0.0498 - 4s/epoch - 38ms/step\n",
      "Epoch 8/100\n",
      "109/109 - 4s - loss: 0.0214 - val_loss: 0.0496 - 4s/epoch - 40ms/step\n",
      "Epoch 9/100\n",
      "109/109 - 4s - loss: 0.0213 - val_loss: 0.0491 - 4s/epoch - 36ms/step\n",
      "Epoch 10/100\n",
      "109/109 - 4s - loss: 0.0212 - val_loss: 0.0489 - 4s/epoch - 39ms/step\n",
      "Epoch 11/100\n",
      "109/109 - 4s - loss: 0.0212 - val_loss: 0.0490 - 4s/epoch - 39ms/step\n",
      "Epoch 12/100\n",
      "109/109 - 4s - loss: 0.0211 - val_loss: 0.0488 - 4s/epoch - 39ms/step\n",
      "Epoch 13/100\n",
      "109/109 - 4s - loss: 0.0211 - val_loss: 0.0493 - 4s/epoch - 39ms/step\n",
      "Epoch 14/100\n",
      "109/109 - 4s - loss: 0.0211 - val_loss: 0.0489 - 4s/epoch - 37ms/step\n",
      "Epoch 15/100\n",
      "109/109 - 4s - loss: 0.0210 - val_loss: 0.0483 - 4s/epoch - 37ms/step\n",
      "Epoch 16/100\n",
      "109/109 - 4s - loss: 0.0210 - val_loss: 0.0485 - 4s/epoch - 38ms/step\n",
      "Epoch 17/100\n",
      "109/109 - 4s - loss: 0.0209 - val_loss: 0.0483 - 4s/epoch - 40ms/step\n",
      "Epoch 18/100\n",
      "109/109 - 5s - loss: 0.0209 - val_loss: 0.0481 - 5s/epoch - 42ms/step\n",
      "Epoch 19/100\n",
      "109/109 - 4s - loss: 0.0209 - val_loss: 0.0481 - 4s/epoch - 41ms/step\n",
      "Epoch 20/100\n",
      "109/109 - 4s - loss: 0.0209 - val_loss: 0.0481 - 4s/epoch - 41ms/step\n",
      "Epoch 21/100\n",
      "109/109 - 4s - loss: 0.0208 - val_loss: 0.0487 - 4s/epoch - 41ms/step\n",
      "Epoch 22/100\n",
      "109/109 - 4s - loss: 0.0208 - val_loss: 0.0480 - 4s/epoch - 39ms/step\n",
      "Epoch 23/100\n",
      "109/109 - 4s - loss: 0.0208 - val_loss: 0.0481 - 4s/epoch - 40ms/step\n",
      "Epoch 24/100\n",
      "109/109 - 4s - loss: 0.0208 - val_loss: 0.0478 - 4s/epoch - 35ms/step\n",
      "Epoch 25/100\n",
      "109/109 - 4s - loss: 0.0207 - val_loss: 0.0485 - 4s/epoch - 35ms/step\n",
      "Epoch 26/100\n",
      "109/109 - 4s - loss: 0.0208 - val_loss: 0.0477 - 4s/epoch - 36ms/step\n",
      "Epoch 27/100\n",
      "109/109 - 4s - loss: 0.0207 - val_loss: 0.0480 - 4s/epoch - 34ms/step\n",
      "Epoch 28/100\n",
      "109/109 - 4s - loss: 0.0207 - val_loss: 0.0475 - 4s/epoch - 38ms/step\n",
      "Epoch 29/100\n",
      "109/109 - 4s - loss: 0.0206 - val_loss: 0.0473 - 4s/epoch - 38ms/step\n",
      "Epoch 30/100\n",
      "109/109 - 4s - loss: 0.0205 - val_loss: 0.0478 - 4s/epoch - 36ms/step\n",
      "Epoch 31/100\n",
      "109/109 - 4s - loss: 0.0205 - val_loss: 0.0473 - 4s/epoch - 37ms/step\n",
      "Epoch 32/100\n",
      "109/109 - 4s - loss: 0.0202 - val_loss: 0.0471 - 4s/epoch - 37ms/step\n",
      "Epoch 33/100\n",
      "109/109 - 4s - loss: 0.0203 - val_loss: 0.0470 - 4s/epoch - 36ms/step\n",
      "Epoch 34/100\n",
      "109/109 - 4s - loss: 0.0201 - val_loss: 0.0464 - 4s/epoch - 38ms/step\n",
      "Epoch 35/100\n",
      "109/109 - 4s - loss: 0.0200 - val_loss: 0.0466 - 4s/epoch - 39ms/step\n",
      "Epoch 36/100\n",
      "109/109 - 4s - loss: 0.0200 - val_loss: 0.0469 - 4s/epoch - 38ms/step\n",
      "Epoch 37/100\n",
      "109/109 - 4s - loss: 0.0198 - val_loss: 0.0461 - 4s/epoch - 40ms/step\n",
      "Epoch 38/100\n",
      "109/109 - 4s - loss: 0.0198 - val_loss: 0.0463 - 4s/epoch - 39ms/step\n",
      "Epoch 39/100\n",
      "109/109 - 4s - loss: 0.0197 - val_loss: 0.0453 - 4s/epoch - 38ms/step\n",
      "Epoch 40/100\n",
      "109/109 - 4s - loss: 0.0196 - val_loss: 0.0455 - 4s/epoch - 38ms/step\n",
      "Epoch 41/100\n",
      "109/109 - 4s - loss: 0.0194 - val_loss: 0.0458 - 4s/epoch - 38ms/step\n",
      "Epoch 42/100\n",
      "109/109 - 4s - loss: 0.0194 - val_loss: 0.0450 - 4s/epoch - 39ms/step\n",
      "Epoch 43/100\n",
      "109/109 - 4s - loss: 0.0192 - val_loss: 0.0451 - 4s/epoch - 39ms/step\n",
      "Epoch 44/100\n",
      "109/109 - 4s - loss: 0.0193 - val_loss: 0.0454 - 4s/epoch - 40ms/step\n",
      "Epoch 45/100\n",
      "109/109 - 4s - loss: 0.0190 - val_loss: 0.0449 - 4s/epoch - 41ms/step\n",
      "Epoch 46/100\n",
      "109/109 - 4s - loss: 0.0190 - val_loss: 0.0449 - 4s/epoch - 39ms/step\n",
      "Epoch 47/100\n",
      "109/109 - 4s - loss: 0.0191 - val_loss: 0.0446 - 4s/epoch - 39ms/step\n",
      "Epoch 48/100\n",
      "109/109 - 4s - loss: 0.0190 - val_loss: 0.0448 - 4s/epoch - 39ms/step\n",
      "Epoch 49/100\n",
      "109/109 - 4s - loss: 0.0187 - val_loss: 0.0447 - 4s/epoch - 38ms/step\n",
      "Epoch 50/100\n",
      "109/109 - 4s - loss: 0.0187 - val_loss: 0.0442 - 4s/epoch - 37ms/step\n",
      "Epoch 51/100\n",
      "109/109 - 4s - loss: 0.0186 - val_loss: 0.0442 - 4s/epoch - 38ms/step\n",
      "Epoch 52/100\n",
      "109/109 - 4s - loss: 0.0185 - val_loss: 0.0433 - 4s/epoch - 38ms/step\n",
      "Epoch 53/100\n",
      "109/109 - 4s - loss: 0.0181 - val_loss: 0.0417 - 4s/epoch - 39ms/step\n",
      "Epoch 54/100\n",
      "109/109 - 4s - loss: 0.0180 - val_loss: 0.0419 - 4s/epoch - 39ms/step\n",
      "Epoch 55/100\n",
      "109/109 - 4s - loss: 0.0178 - val_loss: 0.0428 - 4s/epoch - 39ms/step\n",
      "Epoch 56/100\n",
      "109/109 - 4s - loss: 0.0178 - val_loss: 0.0411 - 4s/epoch - 37ms/step\n",
      "Epoch 57/100\n",
      "109/109 - 4s - loss: 0.0176 - val_loss: 0.0412 - 4s/epoch - 36ms/step\n",
      "Epoch 58/100\n",
      "109/109 - 4s - loss: 0.0174 - val_loss: 0.0408 - 4s/epoch - 37ms/step\n",
      "Epoch 59/100\n",
      "109/109 - 4s - loss: 0.0172 - val_loss: 0.0406 - 4s/epoch - 36ms/step\n",
      "Epoch 60/100\n",
      "109/109 - 4s - loss: 0.0170 - val_loss: 0.0411 - 4s/epoch - 38ms/step\n",
      "Epoch 61/100\n",
      "109/109 - 4s - loss: 0.0167 - val_loss: 0.0401 - 4s/epoch - 37ms/step\n",
      "Epoch 62/100\n",
      "109/109 - 3s - loss: 0.0164 - val_loss: 0.0397 - 3s/epoch - 29ms/step\n",
      "Epoch 63/100\n",
      "109/109 - 4s - loss: 0.0160 - val_loss: 0.0397 - 4s/epoch - 34ms/step\n",
      "Epoch 64/100\n",
      "109/109 - 4s - loss: 0.0159 - val_loss: 0.0399 - 4s/epoch - 34ms/step\n",
      "Epoch 65/100\n",
      "109/109 - 4s - loss: 0.0157 - val_loss: 0.0402 - 4s/epoch - 36ms/step\n",
      "Epoch 66/100\n",
      "109/109 - 4s - loss: 0.0156 - val_loss: 0.0403 - 4s/epoch - 36ms/step\n",
      "Epoch 67/100\n",
      "109/109 - 4s - loss: 0.0156 - val_loss: 0.0396 - 4s/epoch - 36ms/step\n",
      "Epoch 68/100\n",
      "109/109 - 4s - loss: 0.0154 - val_loss: 0.0401 - 4s/epoch - 37ms/step\n",
      "Epoch 69/100\n",
      "109/109 - 4s - loss: 0.0153 - val_loss: 0.0395 - 4s/epoch - 37ms/step\n",
      "Epoch 70/100\n",
      "109/109 - 4s - loss: 0.0153 - val_loss: 0.0395 - 4s/epoch - 36ms/step\n",
      "Epoch 71/100\n",
      "109/109 - 4s - loss: 0.0151 - val_loss: 0.0392 - 4s/epoch - 37ms/step\n",
      "Epoch 72/100\n",
      "109/109 - 4s - loss: 0.0150 - val_loss: 0.0394 - 4s/epoch - 34ms/step\n",
      "Epoch 73/100\n",
      "109/109 - 4s - loss: 0.0152 - val_loss: 0.0394 - 4s/epoch - 37ms/step\n",
      "Epoch 74/100\n",
      "109/109 - 4s - loss: 0.0151 - val_loss: 0.0395 - 4s/epoch - 35ms/step\n",
      "Epoch 75/100\n",
      "109/109 - 4s - loss: 0.0150 - val_loss: 0.0394 - 4s/epoch - 35ms/step\n",
      "Epoch 76/100\n",
      "109/109 - 4s - loss: 0.0149 - val_loss: 0.0391 - 4s/epoch - 36ms/step\n",
      "Epoch 77/100\n",
      "109/109 - 4s - loss: 0.0148 - val_loss: 0.0396 - 4s/epoch - 38ms/step\n",
      "Epoch 78/100\n",
      "109/109 - 4s - loss: 0.0148 - val_loss: 0.0390 - 4s/epoch - 38ms/step\n",
      "Epoch 79/100\n",
      "109/109 - 4s - loss: 0.0147 - val_loss: 0.0414 - 4s/epoch - 37ms/step\n",
      "Epoch 80/100\n",
      "109/109 - 4s - loss: 0.0150 - val_loss: 0.0392 - 4s/epoch - 34ms/step\n",
      "Epoch 81/100\n",
      "109/109 - 4s - loss: 0.0146 - val_loss: 0.0388 - 4s/epoch - 36ms/step\n",
      "Epoch 82/100\n",
      "109/109 - 4s - loss: 0.0146 - val_loss: 0.0387 - 4s/epoch - 37ms/step\n",
      "Epoch 83/100\n",
      "109/109 - 4s - loss: 0.0145 - val_loss: 0.0390 - 4s/epoch - 37ms/step\n",
      "Epoch 84/100\n",
      "109/109 - 4s - loss: 0.0145 - val_loss: 0.0385 - 4s/epoch - 41ms/step\n",
      "Epoch 85/100\n",
      "109/109 - 4s - loss: 0.0143 - val_loss: 0.0384 - 4s/epoch - 40ms/step\n",
      "Epoch 86/100\n",
      "109/109 - 4s - loss: 0.0144 - val_loss: 0.0388 - 4s/epoch - 34ms/step\n",
      "Epoch 87/100\n",
      "109/109 - 4s - loss: 0.0143 - val_loss: 0.0385 - 4s/epoch - 35ms/step\n",
      "Epoch 88/100\n",
      "109/109 - 4s - loss: 0.0145 - val_loss: 0.0391 - 4s/epoch - 33ms/step\n",
      "Epoch 89/100\n",
      "109/109 - 4s - loss: 0.0142 - val_loss: 0.0387 - 4s/epoch - 37ms/step\n",
      "Epoch 90/100\n",
      "109/109 - 4s - loss: 0.0142 - val_loss: 0.0388 - 4s/epoch - 37ms/step\n",
      "Epoch 91/100\n",
      "109/109 - 4s - loss: 0.0142 - val_loss: 0.0384 - 4s/epoch - 36ms/step\n",
      "Epoch 92/100\n",
      "109/109 - 4s - loss: 0.0140 - val_loss: 0.0385 - 4s/epoch - 36ms/step\n",
      "Epoch 93/100\n",
      "109/109 - 4s - loss: 0.0143 - val_loss: 0.0384 - 4s/epoch - 36ms/step\n",
      "Epoch 94/100\n",
      "109/109 - 4s - loss: 0.0141 - val_loss: 0.0383 - 4s/epoch - 36ms/step\n",
      "Epoch 95/100\n",
      "109/109 - 4s - loss: 0.0140 - val_loss: 0.0386 - 4s/epoch - 35ms/step\n",
      "Epoch 96/100\n",
      "109/109 - 4s - loss: 0.0140 - val_loss: 0.0385 - 4s/epoch - 39ms/step\n",
      "Epoch 97/100\n",
      "109/109 - 4s - loss: 0.0140 - val_loss: 0.0391 - 4s/epoch - 37ms/step\n",
      "Epoch 98/100\n",
      "109/109 - 4s - loss: 0.0140 - val_loss: 0.0387 - 4s/epoch - 35ms/step\n",
      "Epoch 99/100\n",
      "109/109 - 4s - loss: 0.0141 - val_loss: 0.0397 - 4s/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n",
      "109/109 - 4s - loss: 0.0139 - val_loss: 0.0378 - 4s/epoch - 37ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def LstmAEModel(layer1=20, layer2=6, encoding_dim=4, n_features=137):\n",
    "    lstm_autoencoder = Sequential()\n",
    "    lstm_autoencoder.add(LSTM(layer1, activation='relu', input_shape=(8, n_features), return_sequences=True))\n",
    "    lstm_autoencoder.add(LSTM(layer2, activation='relu', return_sequences=True))\n",
    "    lstm_autoencoder.add(LSTM(encoding_dim, activation='relu', return_sequences=False))\n",
    "    lstm_autoencoder.add(RepeatVector(8))\n",
    "    lstm_autoencoder.add(LSTM(layer2, activation='relu', return_sequences=True))\n",
    "    lstm_autoencoder.add(LSTM(layer1, activation='relu', return_sequences=True))\n",
    "    lstm_autoencoder.add(TimeDistributed(Dense(n_features, activation='sigmoid')))\n",
    "\n",
    "    lstm_autoencoder.compile(loss='mse', optimizer='adam')\n",
    "    return lstm_autoencoder\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2']\n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "\n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/LSTMAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)\n",
    "        model  = LstmAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = temporalize(train_merge_df)\n",
    "\n",
    "    model = LstmAEModel(layer1=128, layer2=64, encoding_dim=16)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_LstmAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622695f0",
   "metadata": {},
   "source": [
    "## LSTMATTNAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input, LSTM,Dense,RepeatVector, TimeDistributed , LeakyReLU\n",
    "from sklearn.metrics import f1_score, make_scorer, mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input,Conv2D, Conv1D, Conv1DTranspose,MaxPooling1D\n",
    "from tensorflow.keras.layers import  AveragePooling1D, UpSampling1D\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import ConvLSTM1D, Conv1D, BatchNormalization, RepeatVector, TimeDistributed, Reshape,Attention, GlobalMaxPooling1D\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "from tcn import TCN\n",
    "import ast\n",
    "def LstmAttnAEModel(layer1=20, layer2=4, encoding_dim=4, n_features=137):\n",
    "    encoder_inputs = Input(shape=(8, n_features))\n",
    "    encoder = LSTM(layer1, activation='relu', return_sequences=True)(encoder_inputs)\n",
    "    encoder = LSTM(layer2, activation='relu', return_sequences=True)(encoder)\n",
    "    encoder = LSTM(encoding_dim, activation='relu', return_sequences=False)(encoder)\n",
    "    encoder_outputs = RepeatVector(8)(encoder)\n",
    "    attention = Attention()([encoder, encoder_outputs])\n",
    "    attention = GlobalMaxPooling1D()(attention)\n",
    "    attention = Reshape((1, encoding_dim))(attention)\n",
    "    decoder = LSTM(layer2, activation='relu', return_sequences=True)(attention)\n",
    "    decoder = LSTM(layer1, activation='relu', return_sequences=True)(decoder)\n",
    "    decoder_outputs = TimeDistributed(Dense(n_features,activation='sigmoid'))(decoder)\n",
    "    autoencoder = Model(encoder_inputs, decoder_outputs)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder\n",
    "def temporalize(X, lookback=8):\n",
    "\n",
    "    output_X = []\n",
    "    for i in range(len(X) - lookback - 1):\n",
    "        t = []\n",
    "        for j in range(1, lookback + 1):\n",
    "            t.append(X[[(i + j + 1)], :])\n",
    "        output_X.append(t)\n",
    "    return np.squeeze(np.array(output_X))\n",
    "def getHyperParams(file, df_hyp):\n",
    "    param_dict = ast.literal_eval(df_hyp.loc[df_hyp['Inverter'] == file]['Feature Importance'].values[0])\n",
    "    return param_dict['model__batch_size'], param_dict['model__encoding_dim'], param_dict['model__epochs'],param_dict['model__layer1'], param_dict['model__layer2'] \n",
    "\n",
    "def getPIDData():\n",
    "    path_errors = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/errortimestamps.csv'\n",
    "    path_features = '/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/featureslist.csv'\n",
    "    df_errors = pd.read_csv(path_errors)\n",
    "    df_features = pd.read_csv(path_features)\n",
    "    total_features = list(df_features[(df_features['True/False'] == 'TRUE') | (df_features['True/False'] == 'NotAvailable')]['Features'])\n",
    "    tot_feat = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/totfeat.csv')\n",
    "    tot_feat = list(tot_feat['feat'])\n",
    "    xtrain_dfs = {}\n",
    "    xtest_dfs = {}\n",
    "    ytrain_dfs = {}\n",
    "    ytest_dfs = {}\n",
    "    for inverter in range(19):\n",
    "        inv = 'inv_'+str(inverter)\n",
    "        tot_feat.append('ErrBits')\n",
    "        df_inv_0 = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/downsampledata_PandasMeanNew/'+str(inverter)+'.csv')\n",
    "        df_inv_0['Timestamp'] = pd.to_datetime(df_inv_0['Timestamp'])\n",
    "        df_inv_0.sort_values(by='Timestamp', inplace=True)\n",
    "        df_inv_0 = df_inv_0.set_index(df_inv_0['Timestamp'])\n",
    "        df_inv_0.shape\n",
    "\n",
    "\n",
    "        format = '%Y-%m-%d %H:%M:%S'\n",
    "        error_date = datetime.strptime(df_errors[(df_errors['Inverter'] == inverter) & (df_errors['had_failure'] == True)]['failure_time'].values[0][0:19], format)\n",
    "        start_date = error_date-timedelta(days= 44)\n",
    "        end_date = error_date+timedelta(days= 1*30)\n",
    "        split_date = error_date-timedelta(days= 1*30)\n",
    "\n",
    "        df_inv_0 = df_inv_0[(df_inv_0.index > start_date) & (df_inv_0.index < end_date)]\n",
    "        df_inv_0 = pd.get_dummies(df_inv_0)\n",
    "\n",
    "        for feat in tot_feat:\n",
    "            if feat not in df_inv_0.columns:\n",
    "                df_inv_0[feat] = 0\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "        df_inv_0 = df_inv_0.dropna()\n",
    "        df_target = df_inv_0[['ErrBits']]\n",
    "        tot_feat.remove('ErrBits')\n",
    "        df_inv_0 = df_inv_0[tot_feat]\n",
    "\n",
    "        xtrain, xtest = df_inv_0[(df_inv_0.index >= start_date) & (df_inv_0.index< split_date)], df_inv_0[(df_inv_0.index >= split_date) & (df_inv_0.index< end_date)]\n",
    "        ytrain, ytest = df_target[(df_target.index >= start_date) & (df_target.index< split_date)], df_target[(df_target.index >= split_date) & (df_target.index< end_date)]\n",
    "        xtrain_dfs[inv] = xtrain\n",
    "        xtest_dfs[inv] = xtest\n",
    "        ytrain_dfs[inv] = ytrain\n",
    "        ytest_dfs[inv] = ytest\n",
    "    return xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs\n",
    "def get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs):\n",
    "    xtrain, xtest = xtrain_dfs[inv], xtest_dfs[inv]\n",
    "    ytrain, ytest = ytrain_dfs[inv], ytest_dfs[inv]\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(xtrain)\n",
    "    xtrain = X_scaler.transform(xtrain)\n",
    "    xtest = X_scaler.transform(xtest)\n",
    "    X_scalar = MinMaxScaler()\n",
    "    X_scalar.fit(xtrain)\n",
    "    xtrain, xtest = X_scalar.transform(xtrain), X_scalar.transform(xtest)\n",
    "    return xtrain, xtest, ytrain, ytest\n",
    "\n",
    "def other_evaluation(xtrain, xtest, ytest, ytrain, model):\n",
    "    test_enc = model.predict(xtest)\n",
    "    train_enc = model.predict(xtrain)\n",
    "    test_mses = np.mean(np.power(flatten(xtest)- flatten(test_enc), 2), axis=1)\n",
    "    train_mses = np.mean(np.power(flatten(xtrain)- flatten(train_enc), 2), axis=1)\n",
    "    test_maes = np.mean(flatten(xtest)- flatten(test_enc), axis=1)\n",
    "    train_maes = np.mean(flatten(xtrain)- flatten(train_enc), axis=1)\n",
    "    mse_threshold = np.max(train_mses)\n",
    "    mae_threshold = np.max(train_maes)\n",
    "    ypred_mse = np.where(test_mses>mse_threshold,1,0)\n",
    "    ypred_mae = np.where(test_maes>mae_threshold,1,0)\n",
    "    test_df = pd.DataFrame(ytest)\n",
    "    test_df['mse_scores'] = test_mses\n",
    "    test_df['ytrue'] = np.where(test_df['ErrBits']>0, 1, 0)\n",
    "    test_df['mae_scores'] = test_maes\n",
    "    test_df['ypred_mse'] = ypred_mse\n",
    "    test_df['ypred_mae'] = ypred_mae\n",
    "    test_df = test_df[['ytrue','ypred_mse','ypred_mae','mse_scores', 'mae_scores']]\n",
    "    \n",
    "    train_df = pd.DataFrame(ytrain)\n",
    "    train_df['mse_scores'] = train_mses\n",
    "    train_df['ytrue'] = np.where(train_df['ErrBits']>0, 1, 0)\n",
    "    train_df['mae_scores'] = train_maes\n",
    "    train_df = train_df[['ytrue','mse_scores', 'mae_scores']]\n",
    "    return test_df, train_df\n",
    "def data_unimodel(source_list, xtrain_dfs):\n",
    "    train_merge = pd.DataFrame()\n",
    "    for inverter in source_list:\n",
    "        X = xtrain_dfs[f'inv_{inverter}']\n",
    "        X_scaler = MinMaxScaler()\n",
    "        X_scaler.fit(X)\n",
    "        xtrain = X_scaler.transform(X)\n",
    "        df_train = pd.DataFrame(xtrain)\n",
    "        train_merge = pd.concat([train_merge, df_train], axis=0)\n",
    "    train_merge_df = np.array(train_merge)\n",
    "    return train_merge_df\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2])) \n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "source_list = [12, 4, 6, 14, 10, 18, 5, 8, 3, 0, 11, 2]\n",
    "total_list = [0, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 17, 18]\n",
    "\n",
    "xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs = getPIDData()\n",
    "for i in range(1,4):\n",
    "    df_hyp = pd.read_csv('/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/LSTMATTNAE_HYP.csv')\n",
    "    PID_RESULTSPATH_TOTAL_TRAIN = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/train/'\n",
    "    PID_RESULTSPATH_TOTAL_TEST = f'/mnt/work/digitwin/subset_data_for_experiment/PID_2WExp/Results/Iter{i}/test/'\n",
    "    callback1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=1)\n",
    "    lookback = 8\n",
    "    source_dict_results_test =  {}\n",
    "    source_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = f'inv_{file}'\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        model__batch_size,model__encoding_dim, model__epochs, model__layer1, model__layer2 = getHyperParams(file, df_hyp)                                               \n",
    "        model  = LstmAttnAEModel(layer1=model__layer1, layer2=model__layer2, encoding_dim=model__encoding_dim)\n",
    "        model.fit(xtrain, xtrain, epochs=model__epochs, batch_size=model__batch_size, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        source_dict_results_test[file] = test_df\n",
    "        source_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_STL_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_STL_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(source_dict_results_train, f)\n",
    "    train_merge_df = data_unimodel(source_list, xtrain_dfs)\n",
    "    random.shuffle(train_merge_df)\n",
    "    train_merge_df = temporalize(train_merge_df)\n",
    "\n",
    "    model = LstmAttnAEModel(layer1=64, layer2=64, encoding_dim=32)\n",
    "    model.fit(train_merge_df, train_merge_df, epochs=200, batch_size=128, verbose=2,validation_split=0.1, callbacks=callback1)\n",
    "    lookback = 8\n",
    "    mtl_dict_results_test =  {}\n",
    "    mtl_dict_results_train = {}\n",
    "    for file in total_list:\n",
    "        inv = 'inv_'+str(file)\n",
    "        xtrain, xtest, ytrain, ytest = get_train_test_data(inv, xtrain_dfs, xtest_dfs, ytrain_dfs, ytest_dfs)\n",
    "        xtrain = temporalize(xtrain)\n",
    "        xtest = temporalize(xtest)\n",
    "        ytrain, ytest = ytrain[lookback+1:], ytest[lookback+1:]\n",
    "        test_df, train_df = other_evaluation(xtrain, xtest, ytest, ytrain, model)\n",
    "        mtl_dict_results_test[file] = test_df\n",
    "        mtl_dict_results_train[file] = train_df\n",
    "    with open(PID_RESULTSPATH_TOTAL_TEST+'test_MTL_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_test, f)\n",
    "    with open(PID_RESULTSPATH_TOTAL_TRAIN+'train_MTL_LstmAttnAE.pkl', 'wb') as f:\n",
    "        pickle.dump(mtl_dict_results_train, f)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
